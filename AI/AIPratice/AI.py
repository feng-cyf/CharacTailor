import os
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
import pandas as pd
import json
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error
from transformers import BertTokenizer, BertModel, get_linear_schedule_with_warmup


# ====================== é…ç½®å‚æ•°ï¼ˆä¼˜åŒ–ç‰ˆï¼‰=====================
class Config:
    bert_path = "bert-base-chinese"  # è‡ªåŠ¨ä¸‹è½½å®˜æ–¹æ¨¡å‹
    save_model_path = "D:\\GridFriend\\AI\\LoadModels\\best_emotion_model.pth"
    max_len = 128  # æ–‡æœ¬æœ€å¤§é•¿åº¦
    batch_size = 32  # é™ä½batch_sizeï¼Œæå‡ç¨³å®šæ€§
    epochs = 10  # å¢åŠ è®­ç»ƒè½®æ¬¡
    lr = 2e-5  # é™ä½å­¦ä¹ ç‡ï¼Œé¿å…éœ‡è¡
    weight_decay = 1e-4  # æƒé‡è¡°å‡é˜²è¿‡æ‹Ÿåˆ
    warmup_ratio = 0.2  # å»¶é•¿é¢„çƒ­æ—¶é—´
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    emotion_num = 7  # æƒ…ç»ªç±»åˆ«æ•°
    trend_num = 4  # è¶‹åŠ¿ç±»åˆ«æ•°
    intent_num = 8  # æ„å›¾ç±»åˆ«æ•°ï¼ˆå«æ–°å¢çš„"æ±‚å›åº”"ï¼‰


config = Config()

# ====================== æ ‡ç­¾æ˜ å°„ï¼ˆç¡®ä¿å®Œæ•´ï¼‰=====================
emotion2id = {
    "ç”œèœœ": 0, "æ’’å¨‡": 1, "å§”å±ˆ": 2, "å°ç”Ÿæ°”": 3,
    "å¿ƒåŠ¨": 4, "æ€å¿µ": 5, "ä¸­æ€§": 6
}
trend2id = {"ä¸Šå‡": 0, "ä¸‹é™": 1, "å¹³ç¨³": 2, "æ³¢åŠ¨": 3}
intent2id = {
    "æ±‚äº’åŠ¨": 0, "æ±‚å…³æ³¨": 1, "æ±‚å®‰æ…°": 2, "æ±‚è§é¢": 3,
    "è¡¨è¾¾ä¸æ»¡": 4, "è¡¨è¾¾": 5, "æ’’å¨‡è°ƒä¾ƒ": 6, "æ±‚å›åº”": 7
}

label_mappings = {
    "emotion2id": emotion2id,
    "id2emotion": {v: k for k, v in emotion2id.items()},
    "trend2id": trend2id,
    "id2trend": {v: k for k, v in trend2id.items()},
    "intent2id": intent2id,
    "id2intent": {v: k for k, v in intent2id.items()}
}


# ====================== æ•°æ®é›†ç±»ï¼ˆä¿®å¤æ–‡æœ¬å¤„ç†ï¼‰=====================
class EmotionalDataset(Dataset):
    def __init__(self, data, tokenizer, label_mappings):
        self.data = data
        self.tokenizer = tokenizer
        self.emotion2id = label_mappings["emotion2id"]
        self.trend2id = label_mappings["trend2id"]
        self.intent2id = label_mappings["intent2id"]

    def __len__(self):
        return len(self.data)

    def __getitem__(self, idx):
        row = self.data.iloc[idx]
        trigger_event = row["trigger_event"]

        # ç¨³å®šè§£æä¸Šä¸‹æ–‡å’Œå½“å‰æ–‡æœ¬ï¼ˆå®¹é”™å¤„ç†ï¼‰
        if " | å½“å‰æ–‡æœ¬ï¼š" in trigger_event:
            context_part, current_text = trigger_event.split(" | å½“å‰æ–‡æœ¬ï¼š", 1)
            try:
                context = json.loads(context_part.replace("ä¸Šä¸‹æ–‡ï¼š", ""))
            except:
                context = []  # è§£æå¤±è´¥æ—¶ç”¨ç©ºä¸Šä¸‹æ–‡
        else:
            context = []
            current_text = trigger_event  # æ ¼å¼é”™è¯¯æ—¶ç›´æ¥ç”¨åŸæ–‡

        # å¢å¼ºæ–‡æœ¬æ ¼å¼ï¼Œçªå‡ºä¸Šä¸‹æ–‡ä¸å½“å‰æ–‡æœ¬çš„åŒºåˆ«
        text = "[CTX] " + " [SEP] ".join(context) + " [SEP] [CUR] " + current_text

        # æ–‡æœ¬ç¼–ç 
        encoding = self.tokenizer(
            text,
            max_length=config.max_len,
            padding="max_length",
            truncation=True,
            return_tensors="pt"
        )
        input_ids = encoding["input_ids"].flatten()
        attention_mask = encoding["attention_mask"].flatten()

        # æ ‡ç­¾ç¼–ç 
        emotion_id = torch.tensor(self.emotion2id[row["emotion_label"]], dtype=torch.long)
        trend_id = torch.tensor(self.trend2id[row["emotion_trend"]], dtype=torch.long)
        intent_id = torch.tensor(self.intent2id[row["user_intent"]], dtype=torch.long)
        strength = torch.tensor(row["emotion_strength"], dtype=torch.float32)

        return {
            "input_ids": input_ids,
            "attention_mask": attention_mask,
            "emotion": emotion_id,
            "trend": trend_id,
            "intent": intent_id,
            "strength": strength
        }


# ====================== æ¨¡å‹å®šä¹‰ï¼ˆå¢å¼ºå®¹é‡ï¼‰=====================
class ContextEmotionModel(nn.Module):
    def __init__(self, config):
        super().__init__()
        self.bert = BertModel.from_pretrained(config.bert_path)
        self.hidden_size = self.bert.config.hidden_size
        self.dropout = nn.Dropout(0.3)  # æé«˜dropoutå¢å¼ºæ³›åŒ–

        # æ–°å¢å…±äº«ç‰¹å¾å±‚ï¼Œæå‡åˆ†ç±»èƒ½åŠ›
        self.shared_fc = nn.Linear(self.hidden_size, 256)

        # åˆ†ç±»å¤´ï¼ˆåŸºäºå…±äº«ç‰¹å¾ï¼‰
        self.emotion_classifier = nn.Linear(256, config.emotion_num)
        self.trend_classifier = nn.Linear(256, config.trend_num)
        self.intent_classifier = nn.Linear(256, config.intent_num)

        # å¼ºåº¦å›å½’å¤´ï¼ˆåŸºäºå…±äº«ç‰¹å¾ï¼‰
        self.strength_regressor = nn.Sequential(
            nn.Linear(256, 64),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(64, 1)
        )

    def forward(self, input_ids, attention_mask):
        # Bertç¼–ç 
        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)
        cls_emb = outputs.last_hidden_state[:, 0, :]  # [batch_size, hidden_size]
        cls_emb = self.dropout(cls_emb)

        # å…±äº«ç‰¹å¾æå–
        shared_feat = self.shared_fc(cls_emb)
        shared_feat = nn.ReLU()(shared_feat)  # å¢åŠ éçº¿æ€§è¡¨è¾¾

        # åˆ†ç±»è¾“å‡º
        emotion_logits = self.emotion_classifier(shared_feat)
        trend_logits = self.trend_classifier(shared_feat)
        intent_logits = self.intent_classifier(shared_feat)

        # å¼ºåº¦å›å½’è¾“å‡º
        strength_pred = self.strength_regressor(shared_feat).squeeze(-1)

        return emotion_logits, trend_logits, intent_logits, strength_pred


# ====================== è®­ç»ƒå‡½æ•°ï¼ˆä¼˜åŒ–æŸå¤±å’Œæ—©åœï¼‰=====================
def train_model(model, train_loader, val_loader, config):
    # æŸå¤±å‡½æ•°
    ce_loss = nn.CrossEntropyLoss()
    mse_loss = nn.MSELoss()

    # ä¼˜åŒ–å™¨ï¼ˆæ”¹ç”¨PyTorchåŸç”Ÿï¼Œæ¶ˆé™¤è­¦å‘Šï¼‰
    optimizer = optim.AdamW(
        model.parameters(),
        lr=config.lr,
        weight_decay=config.weight_decay
    )

    # å­¦ä¹ ç‡è°ƒåº¦å™¨
    total_steps = len(train_loader) * config.epochs
    scheduler = get_linear_schedule_with_warmup(
        optimizer,
        num_warmup_steps=int(total_steps * config.warmup_ratio),
        num_training_steps=total_steps
    )

    model.to(config.device)
    best_val_emotion_acc = 0.0
    early_stop_count = 0
    early_stop_patience = 3  # å»¶é•¿æ—©åœè€å¿ƒ

    for epoch in range(config.epochs):
        print(f"\n===== Epoch {epoch + 1}/{config.epochs} =====")
        model.train()
        total_loss = 0.0

        for batch_idx, batch in enumerate(train_loader):
            # æ•°æ®ç§»åˆ°è®¾å¤‡
            input_ids = batch["input_ids"].to(config.device)
            attention_mask = batch["attention_mask"].to(config.device)
            emotion = batch["emotion"].to(config.device)
            trend = batch["trend"].to(config.device)
            intent = batch["intent"].to(config.device)
            strength = batch["strength"].to(config.device)

            # å‰å‘ä¼ æ’­
            emotion_logits, trend_logits, intent_logits, strength_pred = model(input_ids, attention_mask)

            # è®¡ç®—æŸå¤±ï¼ˆä¼˜åŒ–æƒé‡ï¼Œä¼˜å…ˆæƒ…ç»ªåˆ†ç±»ï¼‰
            loss_emotion = ce_loss(emotion_logits, emotion)
            loss_trend = ce_loss(trend_logits, trend)
            loss_intent = ce_loss(intent_logits, intent)
            loss_strength = mse_loss(strength_pred, strength)

            # æ€»æŸå¤±ï¼šæ ¸å¿ƒä»»åŠ¡æƒé‡æ›´é«˜
            total_batch_loss = (
                    2.0 * loss_emotion  # æƒ…ç»ªåˆ†ç±»ï¼ˆæ ¸å¿ƒï¼‰
                    + 1.0 * loss_trend  # è¶‹åŠ¿åˆ†ç±»
                    + 1.0 * loss_intent  # æ„å›¾åˆ†ç±»
                    + 0.5 * loss_strength  # å¼ºåº¦å›å½’ï¼ˆè¾…åŠ©ï¼‰
            )

            # åå‘ä¼ æ’­
            optimizer.zero_grad()
            total_batch_loss.backward()
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)  # æ¢¯åº¦è£å‰ª
            optimizer.step()
            scheduler.step()

            total_loss += total_batch_loss.item()

            # æ‰“å°è¿›åº¦
            if (batch_idx + 1) % 50 == 0:
                avg_loss = total_loss / (batch_idx + 1)
                print(f"Batch {batch_idx + 1}, Avg Loss: {avg_loss:.4f}")

        # éªŒè¯
        val_emotion_acc, val_strength_mse = evaluate_model(model, val_loader, config)
        print(f"Val Emotion Acc: {val_emotion_acc:.4f}, Val Strength MSE: {val_strength_mse:.4f}")

        # ä¿å­˜æœ€ä¼˜æ¨¡å‹+æ—©åœåˆ¤æ–­
        if val_emotion_acc > best_val_emotion_acc:
            best_val_emotion_acc = val_emotion_acc
            os.makedirs(os.path.dirname(config.save_model_path), exist_ok=True)
            torch.save(model.state_dict(), config.save_model_path)
            print(f"âœ… ä¿å­˜æœ€ä¼˜æ¨¡å‹ï¼ˆEmotion Acc: {best_val_emotion_acc:.4f}ï¼‰")
            early_stop_count = 0
        else:
            early_stop_count += 1
            print(f"âš ï¸  æ—©åœè®¡æ•°å™¨ï¼š{early_stop_count}/{early_stop_patience}")
            if early_stop_count >= early_stop_patience:
                print(f"ğŸ›‘ è¿ç»­{early_stop_patience}è½®æœªæå‡ï¼Œæå‰åœæ­¢è®­ç»ƒï¼")
                break

    print(f"\nè®­ç»ƒå®Œæˆï¼æœ€ä¼˜éªŒè¯æƒ…ç»ªå‡†ç¡®ç‡ï¼š{best_val_emotion_acc:.4f}")


# ====================== éªŒè¯å‡½æ•°ï¼ˆä¿æŒç¨³å®šï¼‰=====================
def evaluate_model(model, val_loader, config):
    model.eval()
    emotion_correct = 0
    total = 0
    strength_preds = []
    strength_labels = []

    with torch.no_grad():
        for batch in val_loader:
            input_ids = batch["input_ids"].to(config.device)
            attention_mask = batch["attention_mask"].to(config.device)
            emotion = batch["emotion"].to(config.device)
            strength = batch["strength"].to(config.device)

            emotion_logits, _, _, strength_pred = model(input_ids, attention_mask)

            # æƒ…ç»ªå‡†ç¡®ç‡
            emotion_pred = torch.argmax(emotion_logits, dim=1)
            emotion_correct += (emotion_pred == emotion).sum().item()
            total += emotion.size(0)

            # å¼ºåº¦MSE
            strength_preds.extend(strength_pred.cpu().numpy())
            strength_labels.extend(strength.cpu().numpy())

    emotion_acc = emotion_correct / total
    strength_mse = mean_squared_error(strength_labels, strength_preds)
    return emotion_acc, strength_mse


# ====================== ä¸»å‡½æ•°ï¼ˆå¢åŠ æ•°æ®æ£€æŸ¥ï¼‰=====================
def main():
    # 1. åŠ è½½æ•°æ®å¹¶æ£€æŸ¥åˆ†å¸ƒ
    print("ğŸ“¥ åŠ è½½æ•°æ®...")
    df = pd.read_csv("emotional_history_train_data.csv")

    # æ£€æŸ¥æƒ…ç»ªæ ‡ç­¾åˆ†å¸ƒï¼ˆç¡®ä¿æ— æç«¯ä¸å¹³è¡¡ï¼‰
    print("\næƒ…ç»ªæ ‡ç­¾åˆ†å¸ƒï¼ˆå æ¯”ï¼‰ï¼š")
    print(df["emotion_label"].value_counts(normalize=True).round(3))

    # 2. åˆ†å±‚æ‹†åˆ†è®­ç»ƒé›†å’ŒéªŒè¯é›†ï¼ˆä¿æŒåˆ†å¸ƒä¸€è‡´ï¼‰
    train_df, val_df = train_test_split(
        df,
        test_size=0.1,
        random_state=42,
        stratify=df["emotion_label"]  # æŒ‰æƒ…ç»ªæ ‡ç­¾åˆ†å±‚
    )

    # 3. åˆå§‹åŒ–Tokenizer
    tokenizer = BertTokenizer.from_pretrained(config.bert_path)

    # 4. åˆ›å»ºæ•°æ®é›†å’ŒDataLoaderï¼ˆå…³é—­å¤šè¿›ç¨‹ï¼Œé¿å…Windowsä¸‹æŠ¥é”™ï¼‰
    train_dataset = EmotionalDataset(train_df, tokenizer, label_mappings)
    val_dataset = EmotionalDataset(val_df, tokenizer, label_mappings)

    train_loader = DataLoader(
        train_dataset,
        batch_size=config.batch_size,
        shuffle=True,
        num_workers=0  # Windowsä¸‹å»ºè®®è®¾ä¸º0ï¼Œé¿å…å¤šè¿›ç¨‹é”™è¯¯
    )
    val_loader = DataLoader(
        val_dataset,
        batch_size=config.batch_size,
        shuffle=False,
        num_workers=0
    )

    # 5. åˆå§‹åŒ–æ¨¡å‹
    print("\nğŸ”§ åˆå§‹åŒ–æ¨¡å‹...")
    model = ContextEmotionModel(config)

    # 6. å¼€å§‹è®­ç»ƒ
    print("ğŸš€ å¼€å§‹è®­ç»ƒ...")
    train_model(model, train_loader, val_loader, config)


if __name__ == "__main__":
    main()